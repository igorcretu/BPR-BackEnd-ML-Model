{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d479b677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading car URLs...\n",
      "\n",
      "======================================================================\n",
      "ðŸš€ BILBASEN HIGH-SPEED SCRAPER\n",
      "======================================================================\n",
      "Total cars: 29083\n",
      "Workers: 10\n",
      "Batch size: 500\n",
      "Est. time: 19 minutes\n",
      "======================================================================\n",
      "\n",
      "\n",
      "======================================================================\n",
      "BATCH 1: 500 cars | Workers: 10\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Igor Cretu\\AppData\\Local\\Temp\\ipykernel_79096\\1654731012.py:49: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  price_elem = soup.find(text=re.compile(r'\\d+[.,]\\d+\\s*kr', re.I))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  100/500 | 0.8 cars/s | ETA: 8m\n",
      "  200/500 | 0.8 cars/s | ETA: 6m\n",
      "  300/500 | 0.8 cars/s | ETA: 4m\n",
      "  400/500 | 0.8 cars/s | ETA: 2m\n",
      "  500/500 | 0.8 cars/s | ETA: 0m\n",
      "âœ… Batch done in 10.2m (0.8 cars/s)\n",
      "ðŸ’¾ Saved: scraped_data/progress_500.csv\n",
      "\n",
      "======================================================================\n",
      "BATCH 2: 500 cars | Workers: 10\n",
      "======================================================================\n",
      "  100/500 | 0.7 cars/s | ETA: 10m\n",
      "  200/500 | 0.7 cars/s | ETA: 7m\n",
      "  300/500 | 0.8 cars/s | ETA: 4m\n",
      "  400/500 | 0.8 cars/s | ETA: 2m\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Bilbasen High-Speed Parallel Scraper\n",
    "With explicit permission from bilbasen.dk\n",
    "\"\"\"\n",
    "\n",
    "import concurrent.futures\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime\n",
    "import os\n",
    "import re\n",
    "\n",
    "# ===== CONFIGURATION =====\n",
    "NUM_WORKERS = 10\n",
    "BATCH_SIZE = 500\n",
    "OUTPUT_DIR = \"scraped_data\"\n",
    "REQUEST_DELAY = 0.5  # With permission, can be aggressive\n",
    "\n",
    "def setup_driver():\n",
    "    \"\"\"Setup headless Chrome driver.\"\"\"\n",
    "    options = Options()\n",
    "    options.add_argument('--headless')\n",
    "    options.add_argument('--no-sandbox')\n",
    "    options.add_argument('--disable-dev-shm-usage')\n",
    "    options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "    options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')\n",
    "    \n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "    driver.set_page_load_timeout(15)\n",
    "    return driver\n",
    "\n",
    "def extract_car_details(soup, url):\n",
    "    \"\"\"Extract comprehensive car details.\"\"\"\n",
    "    car_data = {'url': url, 'scraped_at': datetime.now().isoformat()}\n",
    "    \n",
    "    try:\n",
    "        # Basic info\n",
    "        title = soup.find('h1')\n",
    "        car_data['title'] = title.get_text(strip=True) if title else ''\n",
    "        \n",
    "        # Price\n",
    "        price_elem = soup.find(text=re.compile(r'\\d+[.,]\\d+\\s*kr', re.I))\n",
    "        car_data['price'] = price_elem.strip() if price_elem else ''\n",
    "        \n",
    "        # Description\n",
    "        desc = soup.find('div', class_=re.compile(r'description', re.I))\n",
    "        if desc:\n",
    "            car_data['beskrivelse'] = re.sub(r'\\s+', ' ', desc.get_text(separator=' ', strip=True))\n",
    "        else:\n",
    "            car_data['beskrivelse'] = ''\n",
    "        \n",
    "        # Extract all dt/dd pairs (Details)\n",
    "        for dt, dd in zip(soup.find_all('dt'), soup.find_all('dd')):\n",
    "            key = dt.get_text(strip=True).replace(':', '').replace(' ', '_').lower()\n",
    "            value = dd.get_text(strip=True)\n",
    "            car_data[f'detail_{key}'] = value\n",
    "        \n",
    "        # Equipment list\n",
    "        equipment = []\n",
    "        for li in soup.find_all('li'):\n",
    "            text = li.get_text(strip=True)\n",
    "            if text and len(text) < 100:\n",
    "                equipment.append(text)\n",
    "        car_data['udstyr'] = ' | '.join(equipment) if equipment else ''\n",
    "        \n",
    "    except Exception as e:\n",
    "        car_data['extraction_error'] = str(e)\n",
    "    \n",
    "    return car_data\n",
    "\n",
    "def scrape_single_car(url, worker_id):\n",
    "    \"\"\"Scrape one car (called by parallel workers).\"\"\"\n",
    "    driver = setup_driver()\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        time.sleep(REQUEST_DELAY)\n",
    "        \n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        return extract_car_details(soup, url)\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {'url': url, 'error': str(e)}\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "def scrape_batch_parallel(urls, batch_num, num_workers):\n",
    "    \"\"\"Scrape batch with parallel workers.\"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"BATCH {batch_num}: {len(urls)} cars | Workers: {num_workers}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    start = time.time()\n",
    "    results = []\n",
    "    \n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "        futures = {executor.submit(scrape_single_car, url, i % num_workers): url \n",
    "                  for i, url in enumerate(urls)}\n",
    "        \n",
    "        for i, future in enumerate(concurrent.futures.as_completed(futures), 1):\n",
    "            results.append(future.result())\n",
    "            if i % 100 == 0:\n",
    "                elapsed = time.time() - start\n",
    "                rate = i / elapsed\n",
    "                eta = (len(urls) - i) / rate if rate > 0 else 0\n",
    "                print(f\"  {i}/{len(urls)} | {rate:.1f} cars/s | ETA: {eta/60:.0f}m\")\n",
    "    \n",
    "    elapsed = time.time() - start\n",
    "    print(f\"âœ… Batch done in {elapsed/60:.1f}m ({len(results)/elapsed:.1f} cars/s)\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def main(car_urls_file='bilbasen_car_urls.csv', num_workers=8, batch_size=1000):\n",
    "    \"\"\"Main execution.\"\"\"\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    \n",
    "    # Load URLs\n",
    "    print(\"Loading car URLs...\")\n",
    "    df_urls = pd.read_csv(car_urls_file)\n",
    "    urls = df_urls['url'].tolist()\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"ðŸš€ BILBASEN HIGH-SPEED SCRAPER\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Total cars: {len(urls)}\")\n",
    "    print(f\"Workers: {num_workers}\")\n",
    "    print(f\"Batch size: {batch_size}\")\n",
    "    print(f\"Est. time: {len(urls)/(num_workers*2.5)/60:.0f} minutes\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    all_results = []\n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    # Process in batches\n",
    "    for i in range(0, len(urls), batch_size):\n",
    "        batch_urls = urls[i:i+batch_size]\n",
    "        batch_num = i//batch_size + 1\n",
    "        \n",
    "        batch_results = scrape_batch_parallel(batch_urls, batch_num, num_workers)\n",
    "        all_results.extend(batch_results)\n",
    "        \n",
    "        # Save progress\n",
    "        df_progress = pd.DataFrame(all_results)\n",
    "        progress_file = f\"{OUTPUT_DIR}/progress_{len(all_results)}.csv\"\n",
    "        df_progress.to_csv(progress_file, index=False)\n",
    "        print(f\"ðŸ’¾ Saved: {progress_file}\")\n",
    "    \n",
    "    # Final save\n",
    "    df_final = pd.DataFrame(all_results)\n",
    "    final_file = f\"{OUTPUT_DIR}/bilbasen_complete_{len(all_results)}_cars.csv\"\n",
    "    df_final.to_csv(final_file, index=False)\n",
    "    \n",
    "    # Summary\n",
    "    elapsed = datetime.now() - start_time\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"ðŸŽ‰ COMPLETE!\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Cars: {len(all_results)}\")\n",
    "    print(f\"Time: {elapsed}\")\n",
    "    print(f\"Rate: {len(all_results)/elapsed.total_seconds():.2f} cars/s\")\n",
    "    print(f\"File: {final_file}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Customize here\n",
    "    main(\n",
    "        car_urls_file='bilbasen_car_urls.csv',\n",
    "        num_workers=10,        # Adjust based on your CPU\n",
    "        batch_size=500\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
